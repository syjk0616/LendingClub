{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import Data_Helper as DH\n",
    "import lendingclub\n",
    "import prediction\n",
    "import investment\n",
    "import backtester\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Initiate Config, Data, LendingClub Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize config object\n",
    "config = lendingclub.ConfigData(\"config_data_dummy.ini\")\n",
    "\n",
    "# initialize lendingclup api object\n",
    "lc = lendingclub.LendingClub(config)\n",
    "\n",
    "# initialize data transformer\n",
    "transformer = DH.Transformer_full()\n",
    "\n",
    "# initialize DataHelper\n",
    "periodStart = (\"Q1\", \"2014\")\n",
    "periodEnd = (\"Q2\", \"2016\")\n",
    "DataHelper = DH.DataHelper(periodStart, periodEnd, transformer, lc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set training data\n",
    "DataHelper.set_training_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set test data\n",
    "DataHelper.set_test_dataset([\"LoanStats_2016Q3.csv\", \"LoanStats_2016Q4.csv\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Initiate the fitted models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate the Logistic model from the saved file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"temp_logit.sav\" #\"lr_classification_20190502.sav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate from beginning\n",
    "lr = prediction.ModelLogistic(filename)\n",
    "\n",
    "# update from the saved model\n",
    "lr.set_model_from_file()\n",
    "\n",
    "# check model description\n",
    "lr.model_description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate the Random Forest from the saved file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = \"rf_classification_20190502.sav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate from beginning\n",
    "# rf = prediction.ModelRandomForest(filename)\n",
    "\n",
    "# update from the saved model\n",
    "# rf.set_model_from_file()\n",
    "\n",
    "# check model description\n",
    "# rf.model_description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate the XGB model from the saved file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"xgb_classification_20190502.sav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is loaded from xgb_classification_20190502.sav\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'XGBoost Classification Object, time: 05/04/2019, eta: 0.01, num_rounds: 3651, max_depth: 6, subsample: 0.5'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate from beginning\n",
    "xgb = prediction.ModelXGBClassfication(filename)\n",
    "\n",
    "# update from the saved model\n",
    "xgb.set_model_from_file()\n",
    "\n",
    "# check model description\n",
    "xgb.model_description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Run model on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_score_and_bucket(model, df, n_bucket):\n",
    "    \n",
    "    # get prediction\n",
    "    pred = model.predict_model(df)\n",
    "\n",
    "    # get length\n",
    "    n = len(pred)\n",
    "\n",
    "    # combine prediction prob to dataset\n",
    "    df = model.clean_data(df)\n",
    "    df[\"score\"] = [x[1] for x in pred]\n",
    "    df = df.sort_values(\"score\")\n",
    "    \n",
    "    # add bucket\n",
    "    inc = n//n_bucket\n",
    "    bar = [i*inc for i in range(1,n_bucket+1)]\n",
    "    bar[-1] = bar[-1] + n%n_bucket\n",
    "    \n",
    "    index = np.array(list(range(0,n)))\n",
    "    f = np.vectorize(lambda x: len(bar) - sum([x<=i for i in bar])+1)\n",
    "    buckets = f(index)\n",
    "    \n",
    "    df[\"bucket\"] = buckets\n",
    "    cut = df[[\"bucket\",\"score\"]].groupby([\"bucket\"]).agg(\"max\")\n",
    "    \n",
    "    return n, df, cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bucket = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get training/test for this model\n",
    "training, test = lr.get_data_for_model(DataHelper.training, DataHelper.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get score\n",
    "score = lr.test_model(test)\n",
    "print(\"test score:\",score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add score and bucket\n",
    "_, df_lr_train, cut_lr_train = add_score_and_bucket(lr, DataHelper.training, n_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add score and bucket\n",
    "_, df_lr_test, cut_lr_test = add_score_and_bucket(lr, DataHelper.test, n_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_lr_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_lr_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get training/test for this model\n",
    "training, test = xgb.get_data_for_model(DataHelper.training, DataHelper.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get score\n",
    "score = xgb.test_model(test)\n",
    "print(\"test score:\",score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add score and bucket\n",
    "_, df_xgb_train, cut_xgb_train = add_score_and_bucket(xgb, DataHelper.training, n_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add score and bucket\n",
    "_, df_xgb_test, cut_xgb_test = add_score_and_bucket(xgb, DataHelper.test, n_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_xgb_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_xgb_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Default Ratio by Score Bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of xgb clearly outperforms other models in the good buckets. Please note that 1~2% difference in AUC can leads to very differnt return in the end. This is why we need to dedicate time to increase the prediction power of underlying machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_def_ratio_by_bucket(**kwargs):\n",
    "    \n",
    "    results = {}\n",
    "    for key, df in kwargs.items():\n",
    "        cnt = df.loc[:,[\"bad_loan\", \"bucket\"]].groupby(\"bucket\").agg(\"sum\")\n",
    "        tot = df.loc[:,[\"bad_loan\", \"bucket\"]].groupby(\"bucket\").agg(\"count\")\n",
    "        rat = cnt/tot\n",
    "        results[key] = rat[\"bad_loan\"]\n",
    "    \n",
    "    results = pd.DataFrame(results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is for training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get default rates by bucket for each model\n",
    "default_results = get_def_ratio_by_bucket(lr = df_lr_train, xgb = df_xgb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show default rates by bucket for each model\n",
    "plt.figure(figsize=(13,8))\n",
    "plt.bar(default_results.index.values-0.2, default_results[\"lr\"], align='center', alpha=0.5)\n",
    "plt.bar(default_results.index.values+0.2, default_results[\"xgb\"], align='center', alpha=0.5)\n",
    "plt.legend([\"lr\",\"xgb\"])\n",
    "plt.ylabel('default rate')\n",
    "plt.title('default rate by score bucket')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get default rates by bucket for each model\n",
    "default_results = get_def_ratio_by_bucket(lr = df_lr_test, xgb = df_xgb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show default rates by bucket for each model\n",
    "plt.figure(figsize=(13,8))\n",
    "plt.bar(default_results.index.values-0.2, default_results[\"lr\"], align='center', alpha=0.5)\n",
    "plt.bar(default_results.index.values+0.2, default_results[\"xgb\"], align='center', alpha=0.5)\n",
    "plt.legend([\"lr\",\"xgb\"])\n",
    "plt.ylabel('default rate')\n",
    "plt.title('default rate by score bucket')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Comparison of Default Rate and Interest Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the default rate and interest rate comparison, only the first few buckets are expected to have positive return. To check the rough expectation of our investments, we compared the default rate and interest rate. The interest rate here is the total 3-year interest rate with equal monthly installment of the loans. The first bucket of xdg model would have highestt payoff as 6.2%. Of course, we need to consider prepayment timing, monthly installmet, etc., to properly calculate the expected payoff, so this is rough estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the observation clearly states that we need to stick to the first few buckets to be profitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_int_rate_by_bucket(**kwargs):\n",
    "    \n",
    "    results = {}\n",
    "    origin = np.array([1-x/36 for x in range(0,37)])\n",
    "    \n",
    "    for key, df in kwargs.items():\n",
    "        # add total interest rate\n",
    "        month_rate = df.int_rate/12\n",
    "        df[\"tot_int_rate\"] = np.array([sum(origin*x) for x in month_rate])\n",
    "        # get average by bucket\n",
    "        rate = df.loc[:,[\"tot_int_rate\", \"bucket\"]].groupby(\"bucket\").agg(\"mean\")\n",
    "        results[key] = rate[\"tot_int_rate\"]\n",
    "    \n",
    "    results = pd.DataFrame(results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get average interest rates by bucket for each model\n",
    "int_results = get_int_rate_by_bucket(lr = df_lr_test, xgb = df_xgb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,8))\n",
    "for name in int_results.columns.values:\n",
    "    plt.plot(default_results.index.values, int_results[name] - default_results[name])\n",
    "plt.hlines(0,xmin=min(default_results.index.values),xmax=max(default_results.index.values))\n",
    "plt.legend(int_results.columns.values)\n",
    "plt.title(\"Expected Return\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,8))\n",
    "legends = []\n",
    "for name in int_results.columns.values:\n",
    "    plt.plot(default_results.index.values, int_results[name])\n",
    "    plt.plot(default_results.index.values, default_results[name])\n",
    "    legends.append(name + \": int_rate\")\n",
    "    legends.append(name + \": def_rate\")\n",
    "\n",
    "plt.legend(legends)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in default_results.columns.values:\n",
    "    default_results[name + \"_int\"] = int_results[name]\n",
    "    default_results[name + \"_diff\"] =  default_results[name + \"_int\"] - default_results[name]\n",
    "\n",
    "default_results.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note the number of loans in the first bucket issued during two quarters are sufficient to diversify the investment, since we can invest more than $4,7000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"we can invest more than {} amount in first bucket during two quarters.\".\n",
    "      format(df_xgb_test.shape[0]//n_bucket*25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Heatmap - distribution of Grade by score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First few buckets only include A-grades and B1 grade loans. Our model further differentiate loans among A-grade groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades = list(df_xgb_test[\"sub_grade\"].unique())\n",
    "grades.sort()\n",
    "buckets = list(range(1,n_bucket+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get counts by grade and bucket\n",
    "heatmap = np.zeros([len(grades),n_bucket])\n",
    "groups = df_xgb_test.loc[:,[\"sub_grade\", \"bucket\",\"bad_loan\"]].groupby([\"bucket\",\"sub_grade\"])\n",
    "\n",
    "for group in groups:\n",
    "    b = buckets.index(group[0][0])\n",
    "    g = grades.index(group[0][1])\n",
    "    count = len(group[1])    \n",
    "    heatmap[g,b] = count  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ratio\n",
    "heatmap = heatmap.transpose()\n",
    "heatmap = np.array([x/sum(x) for x in heatmap])\n",
    "heatmap = heatmap.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change it to df\n",
    "heatmap_df = pd.DataFrame(heatmap,index=grades,columns=buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "ax=sns.heatmap(heatmap_df, cmap=\"YlGnBu\")\n",
    "ax.set_title(\"grade ratio by each bucket - xgb\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
